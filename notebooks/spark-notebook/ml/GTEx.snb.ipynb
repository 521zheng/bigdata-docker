{
  "metadata" : {
    "id" : "17645b40-3842-4691-99de-7b8bf28cfd03",
    "name" : "GTEx",
    "user_save_timestamp" : "2018-01-20T01:59:01.295Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : "/root/.coursier/cache/v1",
    "customRepos" : [
      "comp-bio-aging % default % https://dl.bintray.com/comp-bio-aging/main/",
      "denigma % default % https://dl.bintray.com/denigma/denigma-releases/",
      "releases % default % https://oss.sonatype.org/content/repositories/releases",
      "snapshots % default % https://oss.sonatype.org/content/repositories/snapshots/"
    ],
    "customDeps" : [
      "org.bdgenomics.adam %% adam-core-spark2 % 0.23.0",
      "comp.bio.aging %% adam-playground % 0.0.9",
      "com.crealytics %% spark-excel % 0.9.9",
      "org.typelevel %% frameless-cats    % 0.5.0",
      "org.typelevel %% frameless-dataset   % 0.5.0",
      "com.github.pathikrit  %% better-files  % 3.4.0",
      "org.apache.hadoop % hadoop-azure % 2.7.5"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.master" : "spark://spark-master:7077",
      "spark.executor.cores" : "8",
      "spark.executor.memory" : "15G",
      "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
      "spark.kryo.registrator" : "org.bdgenomics.adam.serialization.ADAMKryoRegistrator",
      "spark.kryo.referenceTracking" : "true"
    },
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "99C31FAA4900457583349F96F5418FA8"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.{DataFrame, Encoders, SparkSession}\n",
        "import org.apache.spark.sql.types.StructType\n",
        "import org.bdgenomics.adam.rdd.ADAMContext._\n",
        "import org.bdgenomics.adam.rdd.ADAMContextExtensions._\n",
        "import scala.reflect.runtime.universe._\n",
        "import comp.bio.aging.playground.extras.uniprot._\n",
        "import org.apache.spark.storage.StorageLevel"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.sql.{DataFrame, Encoders, SparkSession}\nimport org.apache.spark.sql.types.StructType\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.adam.rdd.ADAMContextExtensions._\nimport scala.reflect.runtime.universe._\nimport comp.bio.aging.playground.extras.uniprot._\nimport org.apache.spark.storage.StorageLevel\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 2.160s, at 2018-05-16 13:37"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "5E7802CB71DF434988DF1E056C42DF1B"
      },
      "cell_type" : "code",
      "source" : [
        "def sparkHadoopConf(sc: SparkContext, acountName: String, accountKey: String) = {\n",
        "  sc.hadoopConfiguration.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
        "  sc.hadoopConfiguration.set(\"fs.azure.account.key.\" + acountName + \".blob.core.windows.net\", accountKey)\n",
        "  sc\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "sparkHadoopConf: (sc: org.apache.spark.SparkContext, acountName: String, accountKey: String)org.apache.spark.SparkContext\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.068s, at 2018-05-16 13:37"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "3E39E958A500473F80EE396A2666C442"
      },
      "cell_type" : "code",
      "source" : [
        "def azurize(container: String, accountName: String, blobFile: String): String = \"wasbs://\"+container+\"@\"+accountName+\".blob.core.windows.net/\"+blobFile \n",
        "\n",
        "def writeText2Azure[T]( rdd: RDD[T], container: String, accountName: String, blobFile: String ): String =\n",
        "{\n",
        "  val url = azurize(container, accountName, blobFile)\n",
        "  rdd.saveAsTextFile(url)\n",
        "  url\n",
        "}\n",
        "\n",
        "def writeTsv2Azure( df: DataFrame, container: String, accountName: String, blobFile: String ): String =\n",
        "{\n",
        "  val url = azurize(container, accountName, blobFile)\n",
        "  df.write.option(\"sep\",\"\\t\").option(\"header\",\"true\").csv(url)\n",
        "  url\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "azurize: (container: String, accountName: String, blobFile: String)String\nwriteText2Azure: [T](rdd: org.apache.spark.rdd.RDD[T], container: String, accountName: String, blobFile: String)String\nwriteTsv2Azure: (df: org.apache.spark.sql.DataFrame, container: String, accountName: String, blobFile: String)String\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 3,
          "time" : "Took: 1.014s, at 2018-05-16 13:37"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2DB50761C0864F498B6B8DC65C447EC0"
      },
      "cell_type" : "code",
      "source" : [
        "val connString = \"DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=;EndpointSuffix=core.windows.net\"\n",
        "val account = \"pipelines1\"\n",
        "val key = \"\"\n",
        "def az(path: String): String = azurize(\"storage\", account, path)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "connString: String = DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=;EndpointSuffix=core.windows.net\naccount: String = pipelines1\nkey: String = \naz: (path: String)String\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 0.890s, at 2018-05-16 13:37"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "266B6BE9C99147EA8AA9196A2986D5E1"
      },
      "cell_type" : "code",
      "source" : [
        "sparkHadoopConf(sparkContext, account, key)\n",
        "  \n",
        "val spark = SparkSession\n",
        "  .builder()\n",
        "  .appName(\"mapping_models\")\n",
        "  .getOrCreate()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@60078d7a\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 1.851s, at 2018-05-16 20:57"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "DB5912BFD24D45BE979587FBC4FEE8A7"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.functions._\n",
        "import spark.implicits._\n",
        "\n",
        "val toDouble = udf[Double, String]( _.toDouble)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "<console>:83: error: object implicits is not a member of package notebook.spark\n       import spark.implicits._\n                    ^\n"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "657861D9E96340DC92C7F170A58AAFA7"
      },
      "cell_type" : "code",
      "source" : [
        "val gtexPath = \"/GTEx\"\n",
        "//val txtPath = az(gtexPath + \"/GTEx_Analysis_2016-01-15_v7_RSEMv1.2.22_transcript_tpm.txt\")\n",
        "//val gctPath = az(gtexPath + \"/GTEx_Analysis_2016-01-15_v7_RNASeQCv1.1.8_gene_tpm.gct\")\n",
        "val genesPath = az(gtexPath + \"/gtex_genes.tsv\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "gtexPath: String = /GTEx\ngenesPath: String = wasbs://storage@pipelines1.blob.core.windows.net//GTEx/gtex_genes.tsv\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 8,
          "time" : "Took: 0.917s, at 2018-05-16 21:05"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "FEC0F3F817414ECF8C5508219C9AE1A8"
      },
      "cell_type" : "code",
      "source" : [
        "def readTSV(path: String, header: Boolean = false, sep: String = \"\\t\"): DataFrame = spark.read\n",
        "    .option(\"sep\", sep)\n",
        "    .option(\"comment\", \"#\")\n",
        "    .option(\"inferSchema\", true)\n",
        "    .option(\"header\", header)\n",
        "    .option(\"ignoreLeadingWhiteSpace\", true)\n",
        "    .option(\"ignoreTrailingWhiteSpace\", true)\n",
        "    .option(\"ignoreTrailingWhiteSpace\", true)\n",
        "    .option(\"maxColumns\", 150000)\n",
        "    .csv(path)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "readTSV: (path: String, header: Boolean, sep: String)org.apache.spark.sql.DataFrame\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 10,
          "time" : "Took: 0.783s, at 2018-05-16 21:06"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "6DCB2E1DAEAC4D2986EA8EB218272117"
      },
      "cell_type" : "code",
      "source" : [
        "val genes = readTSV(genesPath, true)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "genes: org.apache.spark.sql.DataFrame = [Name: string, ENSG00000223972.4: string ... 56201 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 11,
          "time" : "Took: 1m0.685s, at 2018-05-16 21:08"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "0C043D34E778400386EE747AA674F9BC"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "66BD503CCC4B4D008CDA0042243ED7A8"
      },
      "cell_type" : "code",
      "source" : [
        "(genes.count, genes.columns.size)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "res21: (Long, Int) = (11689,56203)\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "(11689,56203)"
          },
          "output_type" : "execute_result",
          "execution_count" : 14,
          "time" : "Took: 10m12.737s, at 2018-05-16 21:32"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2EE25CA7A4F34F9BA5591E26A30627F2"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
        "import org.apache.spark.ml.stat.Correlation\n",
        "import org.apache.spark.sql.Row\n",
        "\n",
        "val data = Seq(\n",
        "  Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),\n",
        "  Vectors.dense(4.0, 5.0, 0.0, 3.0),\n",
        "  Vectors.dense(6.0, 7.0, 0.0, 8.0),\n",
        "  Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))\n",
        ")\n",
        "\n",
        "val df = data.map(Tuple1.apply).toDF(\"features\")\n",
        "val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head\n",
        "println(\"Pearson correlation matrix:\\n\" + coeff1.toString)\n",
        "\n",
        "val Row(coeff2: Matrix) = Correlation.corr(df, \"features\", \"spearman\").head\n",
        "println(\"Spearman correlation matrix:\\n\" + coeff2.toString)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "Pearson correlation matrix:\n1.0                   0.055641488407465814  NaN  0.4004714203168137  \n0.055641488407465814  1.0                   NaN  0.9135958615342522  \nNaN                   NaN                   1.0  NaN                 \n0.4004714203168137    0.9135958615342522    NaN  1.0                 \nSpearman correlation matrix:\n1.0                  0.10540925533894532  NaN  0.40000000000000174  \n0.10540925533894532  1.0                  NaN  0.9486832980505141   \nNaN                  NaN                  1.0  NaN                  \n0.40000000000000174  0.9486832980505141   NaN  1.0                  \nimport org.apache.spark.ml.linalg.{Matrix, Vectors}\nimport org.apache.spark.ml.stat.Correlation\nimport org.apache.spark.sql.Row\ndata: Seq[org.apache.spark.ml.linalg.Vector] = List((4,[0,3],[1.0,-2.0]), [4.0,5.0,0.0,3.0], [6.0,7.0,0.0,8.0], (4,[0,3],[9.0,1.0]))\ndf: org.apache.spark.sql.DataFrame = [features: vector]\ncoeff1: org.apache.spark.ml.linalg.Matrix =\n1.0                   0.055641488407465814  NaN  0.4004714203168137\n0.055641488407465814  1.0                   NaN  0.9135958615342522\nNaN                   NaN                   1.0  NaN\n0.4004714203168137    0.9135958615342522    NaN  1.0\ncoeff2: org.apache.spark.ml.linalg.Matrix =\n1.0                  0.10540925533894532  NaN  0.40000000000000174\n0.10540925533894532  1.0                  NaN  0.9486832980505141\nNaN              ..."
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 15,
          "time" : "Took: 15.421s, at 2018-05-16 21:38"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "530D048E4BB64FB086E4C20D64812C30"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [ ]
    }
  ],
  "nbformat" : 4
}