{
  "metadata" : {
    "id" : "096875ac-7b61-4d4a-887b-74029dab2ef6",
    "name" : "homology",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : [
      "comp-bio-aging % default % https://dl.bintray.com/comp-bio-aging/main/",
      "denigma % default % https://dl.bintray.com/denigma/denigma-releases/",
      "releases % default % https://oss.sonatype.org/content/repositories/releases"
    ],
    "customDeps" : [
      "comp.bio.aging %% adam-playground % 0.0.8",
      "com.crealytics %% spark-excel % 0.9.9",
      "org.typelevel %% frameless-cats    % 0.4.0",
      "org.typelevel %% frameless-dataset   % 0.4.0",
      "com.github.pathikrit  %% better-files-akka  % 3.4.0"
    ],
    "customImports" : [ ],
    "customArgs" : null,
    "customSparkConf" : {
      "spark.executor.memory" : "14G",
      "spark.master" : "spark://spark-master:7077",
      "spark.app.name" : "Homology search",
      "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
      "spark.kryo.registrator" : "org.bdgenomics.adam.serialization.ADAMKryoRegistrator",
      "spark.kryoserializer.buffer.mb" : 8,
      "spark.kryo.referenceTracking" : "true",
      "spark.executor.cores" : 4
    },
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "FAD8782743E34438B00EBEB51D87F101"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.{SparkConf, SparkContext}\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import frameless.functions.aggregate._\n",
        "import frameless.TypedDataset\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.bdgenomics.adam.rdd.ADAMContext._\n",
        "import comp.bio.aging.playground.extensions._\n",
        "import org.apache.spark.sql._\n",
        "import scala.util.Try\n",
        "import org.apache.spark.storage.StorageLevel\n",
        "\n",
        "val spark = SparkSession\n",
        "  .builder()\n",
        "  .appName(\"Homology search\")\n",
        "  .getOrCreate()\n",
        "// For implicit conversions like converting RDDs to DataFrames\n",
        "import spark.implicits._\n",
        "spark.sparkContext.setLogLevel(\"WARN\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SparkSession\nimport frameless.functions.aggregate._\nimport frameless.TypedDataset\nimport org.apache.spark.sql.SparkSession\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport comp.bio.aging.playground.extensions._\nimport org.apache.spark.sql._\nimport scala.util.Try\nimport org.apache.spark.storage.StorageLevel\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1b7df6cb\nimport spark.implicits._\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 3.285s, at 2018-01-16 16:59"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "0B8E6736A4994F3E83F02746B1113E66"
      },
      "cell_type" : "code",
      "source" : [
        "val base = \"hdfs://namenode/pipelines\"\n",
        "val diamond = base + \"/diamond\"\n",
        "val blastp = diamond + \"/blastp\"\n",
        "val graywhale = base + \"/GRAY_WHALE\"\n",
        "val trinity = graywhale + \"/Trin_Mitya.Trinity.fasta\"\n",
        "val proteins = graywhale + \"/Trin_Mitya.Trinity.fasta.transdecoder.pep\"\n",
        "val gff = graywhale + \"/Trin_Mitya.Trinity.fasta.transdecoder.gff3\"\n",
        "\n",
        "def readTSV(path: String) = spark.read.option(\"sep\", \"\\t\").csv(path)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "base: String = hdfs://namenode/pipelines\ndiamond: String = hdfs://namenode/pipelines/diamond\nblastp: String = hdfs://namenode/pipelines/diamond/blastp\ngraywhale: String = hdfs://namenode/pipelines/GRAY_WHALE\ntrinity: String = hdfs://namenode/pipelines/GRAY_WHALE/Trin_Mitya.Trinity.fasta\nproteins: String = hdfs://namenode/pipelines/GRAY_WHALE/Trin_Mitya.Trinity.fasta.transdecoder.pep\ngff: String = hdfs://namenode/pipelines/GRAY_WHALE/Trin_Mitya.Trinity.fasta.transdecoder.gff3\nreadTSV: (path: String)org.apache.spark.sql.DataFrame\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.631s, at 2018-01-16 16:59"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "C179B26020F245A2B3CE622E156FF4E5"
      },
      "cell_type" : "code",
      "source" : [
        "val proteinsAdam = graywhale + \"/proteins.adam\"\n",
        "val transcriptsAdam = graywhale + \"/transcripts.adam\"\n",
        "\n",
        "def convert() = {\n",
        "  val transcripts = sparkContext.loadFasta(trinity)\n",
        "  val query = sparkContext.loadFasta(proteins)\n",
        "  query.saveAsParquet(proteinsAdam)\n",
        "}\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "proteinsAdam: String = hdfs://namenode/pipelines/GRAY_WHALE/proteins.adam\ntranscriptsAdam: String = hdfs://namenode/pipelines/GRAY_WHALE/transcripts.adam\nconvert: ()Unit\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 1.218s, at 2018-01-16 16:59"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "4B7E9305CBE14A658BA9C3363255E0F1"
      },
      "cell_type" : "code",
      "source" : [
        "val transcripts = sparkContext.loadParquetContigFragments(transcriptsAdam)\n",
        "val query = sparkContext.loadParquetContigFragments(proteinsAdam)\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "transcripts: org.bdgenomics.adam.rdd.contig.NucleotideContigFragmentRDD = ParquetUnboundNucleotideContigFragmentRDD with 114233 reference sequences\nquery: org.bdgenomics.adam.rdd.contig.NucleotideContigFragmentRDD = ParquetUnboundNucleotideContigFragmentRDD with 32429 reference sequences\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 5,
          "time" : "Took: 3.499s, at 2018-01-16 16:59"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "06023BFF488A4A468E64847103B2BDCB"
      },
      "cell_type" : "markdown",
      "source" : "Now, let's write classes to extract headers from ORF predictions"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "D0785270638C439FB266A010BEFBE0D5"
      },
      "cell_type" : "code",
      "source" : [
        "import scala.util.Try\n",
        "trait ProteinSearch{\n",
        "  def id: String\n",
        "  def e: String\n",
        "}\n",
        "\n",
        "case class BlastResult(id: String, score: Double, e: String) extends ProteinSearch\n",
        "case class PfamResult(domain: String, id: String, e: String) extends ProteinSearch\n",
        "\n",
        "case class ProteinPrediction(transcript: String,\n",
        "                             id: String,\n",
        "                             sequence: String,\n",
        "                             orf_type: String,\n",
        "                             score: Double,\n",
        "                             start: Long,\n",
        "                             end: Long,\n",
        "                             len: Int,\n",
        "                             strand: Char,\n",
        "                             diamondHits: List[BlastResult],\n",
        "                             pfamHits: List[PfamResult]\n",
        "                            )\n",
        "\n",
        "def extractPrediction(description: String, sequence: String): ProteinPrediction = {\n",
        "    val gene::transcript::rest::Nil = description.split(\"::\").toList\n",
        "    val id::orf::tp::len_string::params::tr::Nil = rest.split(' ').filter(_!=\"\").toList\n",
        "    val orf_type: String = tp.substring(tp.indexOf(':') + 1)\n",
        "    val _::etc::Nil = tr.split(':').toList\n",
        "    val str::score_string::other = params.split(',').toList\n",
        "    val strand: Char = if(str.contains(\"-\")) '-' else '+'\n",
        "    val len = Integer.parseInt(\n",
        "      len_string.substring(len_string.indexOf(':') + 1)\n",
        "    )\n",
        "    val (diamondHits, pfamHits) = other.foldLeft((List.empty[BlastResult], List.empty[PfamResult])){\n",
        "      case ((b, p), el) =>\n",
        "        val id::value::e::Nil = el.split('|').toList\n",
        "        Try(value.toDouble).map(\n",
        "          v=>\n",
        "            (BlastResult(id, v, e)::b, p)\n",
        "        ).getOrElse(\n",
        "          (b, PfamResult(id, value, e)::p)\n",
        "        )\n",
        "    }\n",
        "    val span = etc.substring(0, etc.indexOf('('))\n",
        "    val start::end::Nil = span.split('-').map(_.toLong).toList\n",
        "    val score = score_string.substring(score_string.indexOf('=') + 1).toDouble\n",
        "    ProteinPrediction(\n",
        "      transcript, id, sequence, orf_type, score, start, end, len, strand, diamondHits, pfamHits\n",
        "    )\n",
        "  }\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import scala.util.Try\ndefined trait ProteinSearch\ndefined class BlastResult\ndefined class PfamResult\ndefined class ProteinPrediction\nextractPrediction: (description: String, sequence: String)ProteinPrediction\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 6,
          "time" : "Took: 1.601s, at 2018-01-16 17:00"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "E77ED6FA26CD4B7BBB6C5865691460E7"
      },
      "cell_type" : "code",
      "source" : [
        "val predictions = query.rdd\n",
        "  .map(q=>extractPrediction(q.getDescription, q.getSequence))\n",
        "  .filter(p=>p.diamondHits.nonEmpty || p.pfamHits.nonEmpty)\n",
        "  .map(p=>(p.transcript, p))"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "predictions: org.apache.spark.rdd.RDD[(String, ProteinPrediction)] = MapPartitionsRDD[4] at map at <console>:110\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 7,
          "time" : "Took: 3.831s, at 2018-01-16 17:00"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab289731078-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "5FAD376E47F8498DB4DF57519A4CD576"
      },
      "cell_type" : "code",
      "source" : [
        "predictions.first"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "0FD9C1DC126A442C960AA0BAFABCFC5D"
      },
      "cell_type" : "code",
      "source" : [
        "val minky = \"file:////pipelines/indexes/MINKY_WHALE/GCF_000493695.1_BalAcu1.0_rna.fna\"\n",
        "val t = sparkContext.loadFasta(minky)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "minky: String = file:////pipelines/indexes/MINKY_WHALE/GCF_000493695.1_BalAcu1.0_rna.fna\nt: org.bdgenomics.adam.rdd.contig.NucleotideContigFragmentRDD = RDDBoundNucleotideContigFragmentRDD with 37868 reference sequences\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 15,
          "time" : "Took: 25m56.851s, at 2017-12-23 21:24"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "28D827EDC343449884E0884DFE7E7951"
      },
      "cell_type" : "code",
      "source" : [
        "t"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "<console>:69: error: not found: value t\n       t\n       ^\n"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "29C5975649DE401E83F77FAFB8B9881A"
      },
      "cell_type" : "code",
      "source" : [
        "println(\"Hello\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "Hello\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.217s, at 2017-12-23 17:39"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "D0B7A9D97F1B46538E36594D483CCF75"
      },
      "cell_type" : "markdown",
      "source" : "Where:\n------\n\tqseqid means Query Seq - id\n\tqlen means Query sequence length\n\tsseqid means Subject Seq - id\n\tsallseqid means All subject Seq - id(s), separated by a ';'\n\tslen means Subject sequence length\n\tqstart means Start of alignment in query\n\tqend means End of alignment in query\n\tsstart means Start of alignment in subject\n\tsend means End of alignment in subject\n\tqseq means Aligned part of query sequence\n\tsseq means Aligned part of subject sequence\n\tevalue means Expect value\n\tbitscore means Bit score\n\tscore means Raw score\n\tlength means Alignment length\n\tpident means Percentage of identical matches\n\tnident means Number of identical matches\n\tmismatch means Number of mismatches\n\tpositive means Number of positive - scoring matches\n\tgapopen means Number of gap openings\n\tgaps means Total number of gaps\n\tppos means Percentage of positive - scoring matches\n\tqframe means Query frame\n\tbtop means Blast traceback operations(BTOP)\n\tstaxids means unique Subject Taxonomy ID(s), separated by a ';' (in numerical order)\n\tstitle means Subject Title\n\tsalltitles means All Subject Title(s), separated by a '<>'\n\tqcovhsp means Query Coverage Per HSP\n\tqtitle means Query title\n"
    }
  ],
  "nbformat" : 4
}