{
  "metadata" : {
    "id" : "90316530-95bc-4f81-8727-cd8a37aef8de",
    "name" : "go_predictions_full.snb.ipynb",
    "user_save_timestamp" : "2018-04-04T14:40:02.111Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : "/root/.coursier/cache/v1",
    "customRepos" : [
      "comp-bio-aging % default % https://dl.bintray.com/comp-bio-aging/main/",
      "denigma % default % https://dl.bintray.com/denigma/denigma-releases/",
      "releases % default % https://oss.sonatype.org/content/repositories/releases",
      "snapshots % default % https://oss.sonatype.org/content/repositories/snapshots/",
      "AKSW % default % http://maven.aksw.org/repository/snapshots"
    ],
    "customDeps" : [
      "org.bdgenomics.adam %% adam-core-spark2 % 0.23.0",
      "comp.bio.aging %% adam-playground % 0.0.9",
      "com.crealytics %% spark-excel % 0.9.9",
      "org.typelevel %% frameless-cats    % 0.5.0",
      "org.typelevel %% frameless-dataset   % 0.5.0",
      "com.github.pathikrit  %% better-files  % 3.4.0",
      "org.apache.hadoop % hadoop-azure % 2.7.5",
      "net.sansa-stack %% sansa-query-spark % 0.3.1-SNAPSHOT",
      "net.sansa-stack %% sansa-rdf-spark % 0.3.1-SNAPSHOT",
      "net.sansa-stack %% sansa-owl-spark % 0.3.1-SNAPSHOT"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.master" : "spark://spark-master:7077",
      "spark.executor.cores" : "8",
      "spark.executor.memory" : "15G",
      "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
      "spark.kryo.registrator" : "org.bdgenomics.adam.serialization.ADAMKryoRegistrator",
      "spark.kryo.referenceTracking" : "true"
    },
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "99C31FAA4900457583349F96F5418FA8"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.{DataFrame, Encoders, SparkSession}\n",
        "import org.apache.spark.sql.types.StructType\n",
        "import org.apache.spark.storage.StorageLevel\n",
        "\n",
        "import scala.reflect.runtime.universe._\n",
        "import comp.bio.aging.playground.extras.uniprot._\n",
        "import org.bdgenomics.adam.rdd.ADAMContext._\n",
        "import org.bdgenomics.adam.rdd.ADAMContextExtensions._"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.sql.{DataFrame, Encoders, SparkSession}\nimport org.apache.spark.sql.types.StructType\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.adam.rdd.ADAMContextExtensions._\nimport scala.reflect.runtime.universe._\nimport comp.bio.aging.playground.extras.uniprot._\nimport org.apache.spark.storage.StorageLevel\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 2s, at 2018-05-07 13:33"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "5E7802CB71DF434988DF1E056C42DF1B"
      },
      "cell_type" : "code",
      "source" : [
        "def sparkHadoopConf(sc: SparkContext, acountName: String, accountKey: String) = {\n",
        "  sc.hadoopConfiguration.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
        "  sc.hadoopConfiguration.set(\"fs.azure.account.key.\" + acountName + \".blob.core.windows.net\", accountKey)\n",
        "  sc\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "sparkHadoopConf: (sc: org.apache.spark.SparkContext, acountName: String, accountKey: String)org.apache.spark.SparkContext\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 1.075s, at 2018-05-07 13:33"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "3E39E958A500473F80EE396A2666C442"
      },
      "cell_type" : "code",
      "source" : [
        "def azurize(container: String, accountName: String, blobFile: String): String = \"wasbs://\"+container+\"@\"+accountName+\".blob.core.windows.net/\"+blobFile \n",
        "\n",
        "def writeText2Azure[T]( rdd: RDD[T], container: String, accountName: String, blobFile: String ): String =\n",
        "{\n",
        "  val url = azurize(container, accountName, blobFile)\n",
        "  rdd.saveAsTextFile(url)\n",
        "  url\n",
        "}\n",
        "\n",
        "def writeTsv2Azure( df: DataFrame, container: String, accountName: String, blobFile: String ): String =\n",
        "{\n",
        "  val url = azurize(container, accountName, blobFile)\n",
        "  df.write.option(\"sep\",\"\\t\").option(\"header\",\"true\").csv(url)\n",
        "  url\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "azurize: (container: String, accountName: String, blobFile: String)String\nwriteText2Azure: [T](rdd: org.apache.spark.rdd.RDD[T], container: String, accountName: String, blobFile: String)String\nwriteTsv2Azure: (df: org.apache.spark.sql.DataFrame, container: String, accountName: String, blobFile: String)String\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 3,
          "time" : "Took: 1.189s, at 2018-05-07 13:33"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2DB50761C0864F498B6B8DC65C447EC0"
      },
      "cell_type" : "code",
      "source" : [
        "val connString = \"DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=;EndpointSuffix=core.windows.net\"\n",
        "val account = \"pipelines1\"\n",
        "val key = \"\"\n",
        "def az(path: String): String = azurize(\"storage\", account, path)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "connString: String = DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=;EndpointSuffix=core.windows.net\naccount: String = pipelines1\nkey: String = \naz: (path: String)String\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 4,
          "time" : "Took: 1.059s, at 2018-05-07 13:33"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "266B6BE9C99147EA8AA9196A2986D5E1"
      },
      "cell_type" : "code",
      "source" : [
        "sparkHadoopConf(sparkContext, account, key)\n",
        "  \n",
        "val spark = SparkSession\n",
        "  .builder()\n",
        "  .appName(\"mapping_models\")\n",
        "  .getOrCreate()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@10db3fa5\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 5,
          "time" : "Took: 1.265s, at 2018-05-07 13:33"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "DB5912BFD24D45BE979587FBC4FEE8A7"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.functions._\n",
        "import spark.implicits._\n",
        "\n",
        "val toDouble = udf[Double, String]( _.toDouble)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.apache.spark.sql.functions._\nimport spark.implicits._\ntoDouble: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(StringType)))\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 6,
          "time" : "Took: 2.656s, at 2018-05-07 13:33"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "712DD799E41F475B88666BB78B6DEF40"
      },
      "cell_type" : "code",
      "source" : [
        "val base = \"/batch/quant\""
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "base: String = /batch/quant\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 7,
          "time" : "Took: 1.206s, at 2018-05-07 13:33"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "DF9DFEA07D924FA3B8BD73B40C5C3D4B"
      },
      "cell_type" : "code",
      "source" : [
        "val mouse_liver_path = az(base + \"/Mus musculus_totalRNA_GSM2927683_quant.sf\")\n",
        "val mouse_kidney_path = az(base + \"/Mus musculus_totalRNA_GSM2927750_quant.sf\")\n",
        "\n",
        "val human_liver_path = az(base + \"/Homo sapiens_totalRNA_GSM1698568_quant.sf\")\n",
        "val human_kidney_path = az(base + \"/Homo sapiens_totalRNA_GSM1698570_quant.sf\")\n",
        "\n",
        "val cow_liver_path = az(base + \"/Bos taurus_totalRNA_GSM2042593_quant.sf\")\n",
        "val cow_kidney_path = az(base + \"/Bos taurus_totalRNA_GSM2042596_quant.sf\")\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "mouse_liver_path: String = wasbs://storage@pipelines1.blob.core.windows.net//batch/quant/Mus musculus_totalRNA_GSM2927683_quant.sf\nmouse_kidney_path: String = wasbs://storage@pipelines1.blob.core.windows.net//batch/quant/Mus musculus_totalRNA_GSM2927750_quant.sf\nhuman_liver_path: String = wasbs://storage@pipelines1.blob.core.windows.net//batch/quant/Homo sapiens_totalRNA_GSM1698568_quant.sf\nhuman_kidney_path: String = wasbs://storage@pipelines1.blob.core.windows.net//batch/quant/Homo sapiens_totalRNA_GSM1698570_quant.sf\ncow_liver_path: String = wasbs://storage@pipelines1.blob.core.windows.net//batch/quant/Bos taurus_totalRNA_GSM2042593_quant.sf\ncow_kidney_path: String = wasbs://storage@pipelines1.blob.core.windows.net//batch/quant/Bos taurus_totalRNA_GSM2042596_quant.sf\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 8,
          "time" : "Took: 1.036s, at 2018-04-04 14:40"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2BA9453C2045461E8EFCEBBC772A7349"
      },
      "cell_type" : "code",
      "source" : [
        "def load_liver(path: String) = spark.readTSV(path, true).withColumn(\"liver\", toDouble($\"TPM\")).drop(\"TPM\").withColumnRenamed(\"Name\",\"transcript\")\n",
        "def load_kidney(path: String) = spark.readTSV(path, true).withColumn(\"kidney\", toDouble($\"TPM\")).drop(\"TPM\").withColumnRenamed(\"Name\",\"transcript\")\n",
        "def join_liver_kidney(liver: DataFrame, kidney: DataFrame): DataFrame =\n",
        "    liver.join(kidney.withColumnRenamed(\"transcript\", \"kidney_transcript\"), $\"transcript\" === $\"kidney_transcript\")\n",
        "    .select($\"transcript\", $\"liver\", $\"kidney\", ( ($\"liver\" + $\"kidney\") / 2.0 ).as(\"avg_expression\"))    \n",
        "    .sort($\"avg_expression\".desc)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "load_liver: (path: String)org.apache.spark.sql.DataFrame\nload_kidney: (path: String)org.apache.spark.sql.DataFrame\njoin_liver_kidney: (liver: org.apache.spark.sql.DataFrame, kidney: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 9,
          "time" : "Took: 1.021s, at 2018-04-04 14:40"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "051374EC3921488C8F8BDA16A387ABC2"
      },
      "cell_type" : "code",
      "source" : [
        "val mouse = join_liver_kidney(load_liver(mouse_liver_path), load_kidney(mouse_kidney_path))\n",
        "val human = join_liver_kidney(load_liver(human_liver_path), load_kidney(human_kidney_path))\n",
        "val cow = join_liver_kidney(load_liver(cow_liver_path), load_kidney(cow_kidney_path))"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "mouse: org.apache.spark.sql.DataFrame = [transcript: string, liver: double ... 2 more fields]\nhuman: org.apache.spark.sql.DataFrame = [transcript: string, liver: double ... 2 more fields]\ncow: org.apache.spark.sql.DataFrame = [transcript: string, liver: double ... 2 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 10,
          "time" : "Took: 21.078s, at 2018-04-04 14:41"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab1495681407-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "BE330E00EF5D4459830EFBC1CE741542"
      },
      "cell_type" : "code",
      "source" : [
        "/*\n",
        "def simplify(dataFrame: DataFrame): Dataset[(String, Double, Double, Double)] = dataFrame.map{ row=>\n",
        "  val tran = row.getAs[String](\"transcript\")\n",
        "  val tr = tran.substring(0,  Math.min(tran.indexOf('|'), tran.length))\n",
        "  (tr.substring(0, Math.min(tr.indexOf('.'),tr.length)),\trow.getAs[Double](\"liver\"), row.getAs[Double](\"kidney\"), row.getAs[Double](\"avg_expression\"))\n",
        "}\n",
        "*/\n",
        "\n",
        "def simplify(dataFrame: DataFrame): Dataset[(String, Double, Double, Double)] = dataFrame.map{ row=>\n",
        "  val tran = row.getAs[String](\"transcript\")\n",
        "  val tr = tran.substring(0, tran.indexOf('|'))\n",
        "  val transcript =  tr.substring(0, Math.min(tr.indexOf('.'),tr.length))\n",
        "  (transcript,\t(row.getAs[Double](\"liver\"), row.getAs[Double](\"kidney\"), row.getAs[Double](\"avg_expression\")))\n",
        "}.rdd.reduceByKey{ case ((a1, b1, c1), (a2, b2,c2)) => (a1+a2, b1+b2, c1 + c2)}\n",
        "    .map{ case (a, (b ,c, d)) => (a, b ,c ,d)}\n",
        "  .toDS()\n",
        "\n",
        "val human_simple = simplify(human).toDF(\"transcript\", \"liver\", \"kidney\", \"avg_expression\")\n",
        "val mouse_simple = simplify(mouse).toDF(\"transcript\", \"liver\", \"kidney\", \"avg_expression\")\n",
        "val cow_simple = simplify(cow).toDF(\"transcript\", \"liver\", \"kidney\", \"avg_expression\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "simplify: (dataFrame: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[(String, Double, Double, Double)]\nhuman_simple: org.apache.spark.sql.DataFrame = [transcript: string, liver: double ... 2 more fields]\nmouse_simple: org.apache.spark.sql.DataFrame = [transcript: string, liver: double ... 2 more fields]\ncow_simple: org.apache.spark.sql.DataFrame = [transcript: string, liver: double ... 2 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 11,
          "time" : "Took: 8.594s, at 2018-04-04 14:41"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "1637F2D27CCA4709BFDB1695B3FFE642"
      },
      "cell_type" : "code",
      "source" : [
        "val headers = List(\"uniprot_ac\", \"uniprot_id\", \"entrez\", \"refSeq\", \"gi\", \"pdb\", \"go\", \n",
        "  \"uniref100\", \"uniref90\", \"uniref50\", \"uniparc\", \"pir\", \n",
        "  \"taxon\", \"mim\", \"unigene\", \"pubmed\", \"embl\", \"embl_cds\", \n",
        "  \"ensembl\", /*\"ensembl_trs\"*/ \"transcript\", \"ensembl_pro\", \"additional_pubmed\") "
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "headers: List[String] = List(uniprot_ac, uniprot_id, entrez, refSeq, gi, pdb, go, uniref100, uniref90, uniref50, uniparc, pir, taxon, mim, unigene, pubmed, embl, embl_cds, ensembl, transcript, ensembl_pro, additional_pubmed)\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 12,
          "time" : "Took: 4.667s, at 2018-04-04 14:41"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "4EF23DCA223C42968468BBC51A62DC25"
      },
      "cell_type" : "code",
      "source" : [
        "val columns = List(\"transcript\", \"uniref90\", \"go\", \"liver\",\"kidney\", \"avg_expression\", \"uniprot_ac\", \"taxon\", \"uniprot_id\", \"entrez\", \"refSeq\", \"gi\", \"uniparc\", \"pubmed\", \"embl\")\n",
        "\n",
        "def joinMapping(mapping: Dataset[UniprotMapping], df: DataFrame): DataFrame = {\n",
        "  val mdf = mapping.flatMap{ u=> if(u.ensembl_trs == null) Nil else  u.ensembl_trs.split(';').map(trs=>u.copy(ensembl_trs = trs)) }.toDF(headers:_*)\n",
        "  mdf.join(df, \"transcript\").select(columns.head, columns.tail:_*)\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "columns: List[String] = List(transcript, uniref90, go, liver, kidney, avg_expression, uniprot_ac, taxon, uniprot_id, entrez, refSeq, gi, uniparc, pubmed, embl)\njoinMapping: (mapping: org.apache.spark.sql.Dataset[comp.bio.aging.playground.extras.uniprot.UniprotMapping], df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 13,
          "time" : "Took: 5.040s, at 2018-04-04 14:41"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "65BFD372FCB64415851AA957F1D1149D"
      },
      "cell_type" : "code",
      "source" : [
        "val human_tax_id = \"9606\"\n",
        "val mouse_tax_id = \"10090\"\n",
        "val cow_tax_id = \"9913\"\n",
        "\n",
        "val ind = az(\"/indexes/uniprot\")\n",
        "val human_mapping = spark.readTypedTSV[UniprotMapping](ind + \"/HUMAN_9606_idmapping_selected.tab\")\n",
        "val mouse_mapping = spark.readTypedTSV[UniprotMapping](ind + \"/MOUSE_10090_idmapping_selected.tab\")\n",
        "val cow_mapping = spark.readTypedTSV[UniprotMapping](ind + s\"/COW_${cow_tax_id}_idmapping_selected.tab\")\n",
        "\n",
        "val all_mapping =  spark.readTypedTSV[UniprotMapping](\"file:///pipelines/indexes/uniprot/idmapping_selected.tab\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "human_tax_id: String = 9606\nmouse_tax_id: String = 10090\ncow_tax_id: String = 9913\nind: String = wasbs://storage@pipelines1.blob.core.windows.net//indexes/uniprot\nhuman_mapping: org.apache.spark.sql.Dataset[comp.bio.aging.playground.extras.uniprot.UniprotMapping] = [uniprot_ac: string, uniprot_id: string ... 20 more fields]\nmouse_mapping: org.apache.spark.sql.Dataset[comp.bio.aging.playground.extras.uniprot.UniprotMapping] = [uniprot_ac: string, uniprot_id: string ... 20 more fields]\ncow_mapping: org.apache.spark.sql.Dataset[comp.bio.aging.playground.extras.uniprot.UniprotMapping] = [uniprot_ac: string, uniprot_id: string ... 20 more fields]\nall_mapping: org.apache.spark.sql.Dataset[comp.bio.aging.playground.extras.uniprot.UniprotMapping] = [uniprot_ac: string, uniprot_id: string ... 20..."
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 14,
          "time" : "Took: 1.917s, at 2018-03-15 12:47"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "15F15DC8139E4F6AA87326F42EAA3920"
      },
      "cell_type" : "code",
      "source" : [
        "(human_mapping.count, mouse_mapping.count, cow_mapping.count)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "res15: (Long, Long, Long) = (162191,83601,32206)\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "(162191,83601,32206)"
          },
          "output_type" : "execute_result",
          "execution_count" : 15,
          "time" : "Took: 8.313s, at 2018-03-13 01:47"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "48F32AA87FD547BCA5F31EE7D052124B"
      },
      "cell_type" : "code",
      "source" : [
        "import org.bdgenomics.adam.rdd.ADAMContextExtensions._\n",
        "import comp.bio.aging.playground.extras.uniprot._\n",
        "def writeTSV(dataFrame: DataFrame, path: String, header: Boolean = true, sep: String = \"\\t\"): Unit =\n",
        "      dataFrame.write.option(\"sep\", sep).option(\"header\",header).csv(path)\n",
        "//val cow_mapping = all_mapping.filter(u=>u.taxon == cow_tax_id)\n",
        "//writeTSV(cow_mapping.toDF().coalesce(1), ind + s\"/COW_${cow_tax_id}_idmapping_selected.tab\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "import org.bdgenomics.adam.rdd.ADAMContextExtensions._\nimport comp.bio.aging.playground.extras.uniprot._\nwriteTSV: (dataFrame: org.apache.spark.sql.DataFrame, path: String, header: Boolean, sep: String)Unit\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 16,
          "time" : "Took: 0.842s, at 2018-03-13 01:47"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "380D79F49879409683C7A9A84E5C0E66"
      },
      "cell_type" : "code",
      "source" : [
        "//val cow_mapping = all_mapping.filter(u=>u.taxon == cow_tax_id)\n",
        "//writeTSV(cow_mapping.toDF().coalesce(1), ind + s\"/COW_${cow_tax_id}_idmapping_selected.tab\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "cow_mapping: org.apache.spark.sql.Dataset[comp.bio.aging.playground.extras.uniprot.UniprotMapping] = [uniprot_ac: string, uniprot_id: string ... 20 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 15,
          "time" : "Took: 15m50.432s, at 2018-03-13 00:31"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "D4ADE49C32FF4E8B883E39042E6D6EC4"
      },
      "cell_type" : "code",
      "source" : [
        "cow"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "res42: org.apache.spark.sql.DataFrame = [transcript: string, liver: double ... 2 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon0892515877633e29ce9cbffebfd7b48f&quot;,&quot;partitionIndexId&quot;:&quot;anon2bbc29b5e778f752df3470388a3a17ed&quot;,&quot;numPartitions&quot;:1883,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;transcript&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;liver&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;kidney&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;avg_expression&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
          },
          "output_type" : "execute_result",
          "execution_count" : 31,
          "time" : "Took: 2.791s, at 2018-03-13 02:00"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "38E0CD8C9B0E43D3897768DE908FCED7"
      },
      "cell_type" : "code",
      "source" : [
        "val humanExpressions = joinMapping(human_mapping, human_simple)\n",
        "val mouseExpressions = joinMapping(mouse_mapping, mouse_simple)\n",
        "//val cowExpressions = joinMapping(cow_mapping, cow_simple)\n",
        "//(68611,51014)\n",
        "(humanExpressions.count, mouseExpressions.count)\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "humanExpressions: org.apache.spark.sql.DataFrame = [transcript: string, uniref90: string ... 13 more fields]\nmouseExpressions: org.apache.spark.sql.DataFrame = [transcript: string, uniref90: string ... 13 more fields]\nres48: (Long, Long) = (68611,51014)\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "(68611,51014)"
          },
          "output_type" : "execute_result",
          "execution_count" : 35,
          "time" : "Took: 13.214s, at 2018-03-13 02:10"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "534E42A89EF2484FBAEAD6961FC993C3"
      },
      "cell_type" : "code",
      "source" : [
        "human_simple"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "res31: org.apache.spark.sql.DataFrame = [transcript: string, liver: double ... 2 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon3f7630128ad5ab5e38bdf1820fac7eac&quot;,&quot;partitionIndexId&quot;:&quot;anon78213d10b2a624460dc36a8d3a294f5a&quot;,&quot;numPartitions&quot;:3817,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;transcript&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;liver&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;kidney&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;avg_expression&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
          },
          "output_type" : "execute_result",
          "execution_count" : 24,
          "time" : "Took: 5.041s, at 2018-03-13 01:55"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "E8352B383AE54129944177C6DE59D550"
      },
      "cell_type" : "code",
      "source" : [
        "writeTSV(humanExpressions.coalesce(1), az(\"/expressions/transcripts/\" + \"human\"+\"_transcripts_all.tab\"), true)\n",
        "writeTSV(mouseExpressions.coalesce(1), az(\"/expressions/transcripts/\" + \"mouse\"+\"_transcripts_all.tab\"), true)"
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 37,
          "time" : "Took: 29.239s, at 2018-03-13 02:15"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "BBE5D049C94445D78079E865BFF8A1A8"
      },
      "cell_type" : "code",
      "source" : [
        "//val go_inner = spark.readTSV(az(\"/expressions/go/gray_whale_with_bowhead_with_minke_with_NMR_small_full_inner.tsv\"), true)\n",
        "//val go_outer = spark.readTSV(az(\"/expressions/go/gray_whale_with_bowhead_with_minke_with_NMR_small_full_outer.tsv\"), true)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "go_inner: org.apache.spark.sql.DataFrame = [go: string, label: string ... 10 more fields]\ngo_outer: org.apache.spark.sql.DataFrame = [go: string, label: string ... 10 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 2.030s, at 2018-04-05 10:03"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "20E76BAAB9AE46E98BE81AD8A39B7247"
      },
      "cell_type" : "code",
      "source" : [
        "val go_outer_full = spark.readTSV(az(\"/expressions/go/gray_whale_with_bowhead_with_minke_with_NMR_large_full_outer.tsv\"), true)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "go_outer_full: org.apache.spark.sql.DataFrame = [go: string, label: string ... 14 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 7,
          "time" : "Took: 2.211s, at 2018-04-12 01:40"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "62326AF1B751444289F88EC59D9C71FE"
      },
      "cell_type" : "code",
      "source" : [
        "val go_outer_small = spark.readTSV(az(\"/expressions/go/gray_whale_with_bowhead_with_minke_with_NMR_small_full_outer.tsv\"), true)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "go_outer_small: org.apache.spark.sql.DataFrame = [go: string, label: string ... 10 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 11,
          "time" : "Took: 1.539s, at 2018-04-12 01:43"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab515212234-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "7B6098D3D21142279FEA80D40C173CFD"
      },
      "cell_type" : "code",
      "source" : [
        "go_outer_counts.columns.map(v=> \"\\\"\" + v +\"\\\"\").mkString(\", \")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "res64: String = \"go\", \"label\", \"gray_whale_liver\", \"bowhead_whale_liver\", \"minke_liver\", \"NMR_liver\", \"gray_whale_kidney\", \"bowhead_whale_kidney\", \"minke_kidney\", \"NMR_kidney\", \"type\", \"transcripts_count\", \"uniref90_count\", \"taxons_count\", \"gray_whale_avg\"\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "&quot;go&quot;, &quot;label&quot;, &quot;gray_whale_liver&quot;, &quot;bowhead_whale_liver&quot;, &quot;minke_liver&quot;, &quot;NMR_liver&quot;, &quot;gray_whale_kidney&quot;, &quot;bowhead_whale_kidney&quot;, &quot;minke_kidney&quot;, &quot;NMR_kidney&quot;, &quot;type&quot;, &quot;transcripts_count&quot;, &quot;uniref90_count&quot;, &quot;taxons_count&quot;, &quot;gray_whale_avg&quot;"
          },
          "output_type" : "execute_result",
          "execution_count" : 17,
          "time" : "Took: 1.871s, at 2018-04-12 11:12"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "809269B4C6BC40468E6BCCE4670055BB"
      },
      "cell_type" : "code",
      "source" : [
        "val count = udf[Int, String](_.split(\";\").size)\n",
        "val go_outer_counts = go_outer_full.na.fill(\"\")\n",
        "    .withColumn(\"transcripts_count\", count($\"transcript\"))\n",
        "    .withColumn(\"uniref90_count\", count($\"uniref90\"))\n",
        "    .withColumn(\"taxons_count\", count($\"taxon\"))\n",
        "    .drop($\"uniprot_ac\").drop($\"taxon\").drop($\"uniref90\").drop($\"transcript\")\n",
        "    .withColumn(\"gray_whale_avg\", toDouble($\"gray_whale_avg_expression\"))\n",
        "    .drop($\"gray_whale_avg_expression\")    \n",
        "    .sort($\"gray_whale_avg\".desc)\n",
        "    .select(\"go\", \"label\", \"gray_whale_avg\", \"transcripts_count\", \"uniref90_count\", \"taxons_count\",  \"type\", \"gray_whale_liver\", \n",
        "            \"bowhead_whale_liver\", \"minke_liver\", \"NMR_liver\", \"gray_whale_kidney\", \n",
        "            \"bowhead_whale_kidney\", \"minke_kidney\", \"NMR_kidney\"\n",
        "            )\n",
        "    \n",
        "    //$\"gray_whale_avg_expression\"\n",
        "go_outer_counts\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "count: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,IntegerType,Some(List(StringType)))\ngo_outer_counts: org.apache.spark.sql.DataFrame = [go: string, label: string ... 13 more fields]\nres67: org.apache.spark.sql.DataFrame = [go: string, label: string ... 13 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonff7e9f11bbf5f8112d4f2ee44466283e&quot;,&quot;partitionIndexId&quot;:&quot;anon044b33d5b9995d2905c9f26ce77f2374&quot;,&quot;numPartitions&quot;:569,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;go&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;label&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;gray_whale_avg&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;transcripts_count&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;uniref90_count&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;taxons_count&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;type&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;gray_whale_liver&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;bowhead_whale_liver&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;minke_liver&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;NMR_liver&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;gray_whale_kidney&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;bowhead_whale_kidney&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;minke_kidney&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;NMR_kidney&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
          },
          "output_type" : "execute_result",
          "execution_count" : 19,
          "time" : "Took: 5.143s, at 2018-04-12 11:14"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "BFC818CBD9124BCD892719AD3868D499"
      },
      "cell_type" : "code",
      "source" : [
        "go_outer_counts.coalesce(1).write.option(\"sep\",\"\\t\").option(\"header\",\"true\")\n",
        ".csv(az(\"/expressions/go/gray_whale_with_bowhead_with_minke_with_NMR_full_outer_counts.tsv\"))\n"
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 23,
          "time" : "Took: 5.135s, at 2018-04-12 11:22"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2837D4BEFBE64CA6B223D8D5B0F1F664"
      },
      "cell_type" : "code",
      "source" : [
        "def byGo(df: DataFrame, species: String) =\n",
        "  df.select(\"go\", \"liver\", \"kidney\").flatMap{row=>\n",
        "    val g = row.getAs[String](\"go\")\n",
        "    val goes = if(g==null) Array[String]() else g.split(\";\").map(go=>go.trim).filter(go=>go!=\"\")\n",
        "    goes.map{ go =>\n",
        "      go -> (row.getAs[Double](\"liver\"), row.getAs[Double](\"kidney\"))\n",
        "    }\n",
        "  }.rdd.reduceByKey{ case ((l1, k1), (l2, k2)) => (l1 + l2, k1 + k2)}\n",
        "    .map{ case (go, (liver, kidney)) => (go, liver, kidney)}\n",
        "      .toDF(\"go\", s\"${species}_liver\", s\"${species}_kidney\")\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "byGo: (df: org.apache.spark.sql.DataFrame, species: String)org.apache.spark.sql.DataFrame\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 14,
          "time" : "Took: 1.162s, at 2018-03-13 12:36"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "2FE5358FC2BC419D9E001A2C76B416CF"
      },
      "cell_type" : "code",
      "source" : [
        "val goHuman = byGo(humanExpressions, \"human\")\n",
        "val goMouse = byGo(mouseExpressions, \"mouse\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "goHuman: org.apache.spark.sql.DataFrame = [go: string, human_liver: double ... 1 more field]\ngoMouse: org.apache.spark.sql.DataFrame = [go: string, mouse_liver: double ... 1 more field]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 24,
          "time" : "Took: 5.709s, at 2018-03-13 12:48"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "40D20D4892654C3C84C4B29E785CFECD"
      },
      "cell_type" : "code",
      "source" : [
        "val go_inner_ext = go_inner.join(goHuman, Seq(\"go\"), \"inner\").join(goMouse, Seq(\"go\"), \"inner\").sort($\"gray_whale_avg_expression\".desc)\n",
        "val go_outer_ext = go_outer.join(goHuman, Seq(\"go\"), \"full_outer\").join(goMouse, Seq(\"go\"), \"full_outer\").sort($\"gray_whale_avg_expression\".desc)\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "go_inner_ext: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [go: string, label: string ... 14 more fields]\ngo_outer_ext: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [go: string, label: string ... 14 more fields]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 25,
          "time" : "Took: 1.125s, at 2018-03-13 12:48"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "F5C94C0C5A5F421B88B44A3B7ECD55A5"
      },
      "cell_type" : "code",
      "source" : [
        "writeTSV(go_inner_ext.coalesce(1), az(\"/expressions/go/gray_whale_with_bowhead_with_minke_with_NMR_with_human_with_mouse_small_full_inner.tsv\"), true)\n",
        "writeTSV(go_outer_ext.coalesce(1), az(\"/expressions/go/gray_whale_with_bowhead_with_minke_with_NMR_with_human_with_mouse_small_full_outer.tsv\"), true)\n"
      ],
      "outputs" : [
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 26,
          "time" : "Took: 26.006s, at 2018-03-13 12:49"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "E09A396D3149441A9DE51AFFCD6B8760"
      },
      "cell_type" : "code",
      "source" : [
        "//SPLIT GO"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "6DE1E0C962B34C60AE522638753E99C0"
      },
      "cell_type" : "code",
      "source" : [
        "import net.sansa_stack.rdf.spark.io._\n",
        "import org.apache.jena.riot.Lang\n",
        "\n",
        "val input = az(\"/go/go.owl\")\n",
        "\n",
        "val lang = Lang.RDFXML\n",
        "val triples = spark.rdf(lang)(input)\n",
        "triples.take(5).foreach(println(_))"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: java.lang.IndexOutOfBoundsException: Index: 102, Size: 27\nSerialization trace:\nfTargetNamespace (org.apache.xerces.impl.dv.xs.XSSimpleTypeDecl)\nfBase (org.apache.xerces.impl.dv.xs.XSSimpleTypeDecl)\ntypeDeclaration (org.apache.jena.datatypes.xsd.impl.XSDBaseStringType)\ndtype (org.apache.jena.graph.impl.LiteralLabelImpl)\nlabel (org.apache.jena.graph.Node_Literal)\nobj (org.apache.jena.graph.Triple)\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1327)\n  ... 89 elided\n"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "D35CF3518C4048FE81B68ED86D717639"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [ ]
    }
  ],
  "nbformat" : 4
}